# Databricks notebook source
# MAGIC %md
# MAGIC ## Telecommunications Reliability Metrics
# MAGIC
# MAGIC **Telecommunications LTE Architecture**
# MAGIC <br>
# MAGIC
# MAGIC
# MAGIC
# MAGIC The modern telecommunications network consists of the Base Station also known as the **eNodeB (Evolved Node B) for 4G networks** is the hardware that communicates directly with the **UE (User Enitity such as a Mobile Phone)**. The **MME (Mobility Management Entity)** manages the entire process from a cell phone making a connection to a network to a paging message being sent to the mobile phone.  
# MAGIC <img style="margin: auto" src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/Telco_simple.png" width="1200"/>
# MAGIC
# MAGIC **Use Case Overview**
# MAGIC * Telecommunications services collect many different forms of data to observe overall network reliability as well as to predict how best to expand the network to reach more customers. Some typical types of data collected are:
# MAGIC   - **PCMD (Per Call Measurement Data):** granular details of all network processes as MME (Mobility Management Entity) manages processes between the UE and the rest of the network
# MAGIC   - **CDR (Call Detail Records):** high level data describing call and SMS activity with fields such as phone number origin, phone number target, status of call/sms, duration, etc. 
# MAGIC * This data can be collected and used in provide a full view of the health of each cell tower in the network as well as the network as a whole. 
# MAGIC * **Note:** for this demo we will be primarily focused on CDR data but will also have a small sample of what PCMD could look like.
# MAGIC
# MAGIC **Business Impact of Solution**
# MAGIC * **Ease of Scaling:** with large amounts of data being generated by a telecommunications system, Databricks can provide the ability to scale so that the data can be reliably ingested and analyzed.  
# MAGIC * **Greater Network Reliability:** with the ability to monitor and predict dropped communications and more generally network faults, telecommunications providers can ultimately deliver better service for their customers and reduce churn.
# MAGIC
# MAGIC **Full Architecture from Ingestion to Analytics and Machine Learning**
# MAGIC
# MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_full.png" width="1000"/>

# COMMAND ----------

# MAGIC %md
# MAGIC ## Data Ingestion
# MAGIC
# MAGIC **Bronze Layer** 
# MAGIC
# MAGIC * Ingestion here starts with loading CDR and PCMD data directly from S3 using Autoloader. Though in this example JSON files are loaded into S3 from where Autoloader will then ingest these files into the bronze layer, streams from Kafka, Kinesis, etc. are supported by simply changing the "format" option on the read operation.
# MAGIC
# MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_bronze.png
# MAGIC " width="1000"/>

# COMMAND ----------

# MAGIC %md
# MAGIC Note: for demo purpose reading data as table and saved as table in UC, not using streaming, not using pcmd_minute_gold since the foreccast model is only using cdr_hour_gold

# COMMAND ----------

# MAGIC %run ./.setup

# COMMAND ----------

cdr_bronze = (
   spark.read.format("json")
   .load(f"{source_path}/CDR"))

cdr_bronze.write.mode("overwrite").saveAsTable("cdr_bronze")

# COMMAND ----------

pcmd_bronze = (
   spark.read.format("json")
   .load(f"{source_path}/PCMD"))

pcmd_bronze.write.mode("overwrite").saveAsTable("pcmd_bronze")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Joining with Tower Data and Creating the Silver Layer 
# MAGIC
# MAGIC **Silver Layer**
# MAGIC
# MAGIC * In the silver layer, the data is refined removing nulls and duplicates while also joining tower information such as state, longitude, and latitude to allow for geospatial analysis. Stream-static joins are performed to do this with the streaming CDR and PCMD records being joined with static tower information which has been stored previously.
# MAGIC
# MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_silver.png" width="1000"/>

# COMMAND ----------

towers = spark.read.json(f"{source_path}/cell_towers.json.gz")

towers = (
    towers
    .select(
        towers.properties.GlobalID.alias("GlobalID"), 
        towers.properties.LocCity.alias("City"), 
        towers.properties.LocCounty.alias("County"), 
        towers.properties.LocState.alias("State"), 
        towers.geometry.coordinates[0].alias("Longitude"), 
        towers.geometry.coordinates[1].alias("Latitude")))

towers.write.mode("overwrite").saveAsTable("towers")

# COMMAND ----------

towers = spark.table("towers")
cdr_bronze = spark.table("cdr_bronze")

cdr_silver = (
    cdr_bronze
    .join(towers, cdr_bronze.towerId == towers.GlobalID))

cdr_silver.write.mode("overwrite").saveAsTable("cdr_silver")

# COMMAND ----------

pcmd_bronze = spark.table("pcmd_bronze")

pcmd_silver = (
    pcmd_bronze
    .join(towers, pcmd_bronze.towerId == towers.GlobalID))

pcmd_silver.write.mode("overwrite").saveAsTable("pcmd_silver")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Aggregating on Various Time Periods to Create the Gold Layer
# MAGIC With Spark **Structured Streaming** the streaming records can be automatically aggregated with stateful processing. Here the aggregation is done on 1 minute intervals and the KPIs are aggregated accordingly. Any interval can be selected here and larger time window aggregations can be done on a scheduled basis with Databricks **Workflows**. For example, the records that are aggregated here at 1 minute intervals can then be aggregated to hour long intervals with a workflow that runs every hour. 
# MAGIC
# MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_gold.png" width="1000"/>

# COMMAND ----------

cdr_silver = spark.table("cdr_silver")

cdr_minute_gold = (
    cdr_silver
    .groupBy(F.window("event_ts", "1 minute"), "towerId").agg(
        F.count(F.when(F.col("status") == "dropped", True)).alias("dropped"),
        F.count(F.when(F.col("status") == "answered", True)).alias("answered"),
        F.count(F.when(F.col("status") == "missed", True)).alias("missed"),
        F.count(F.when(F.col("type") == "text", True)).alias("text"),
        F.count(F.when(F.col("type") == "call", True)).alias("call"),
        F.count(F.lit(1)).alias("totalRecords_CDR"),
        F.first("window.start").alias("window_start"),
        F.first("Longitude").alias("Longitude"),
        F.first("Latitude").alias("Latitude"),
        F.first("City").alias("City"),
        F.first("County").alias("County"),
        F.first("State").alias("state"))
    .withColumn("date", F.col("window_start"))
    .select(
        "date",
        "towerId",
        "answered",
        "dropped",
        "missed",
        "call",
        "text",
        "totalRecords_CDR",
        "Latitude",
        "Longitude",
        "City",
        "County",
        "State"))

cdr_minute_gold.write.mode("overwrite").saveAsTable("cdr_minute_gold")

# COMMAND ----------

# refer to original dlt pipeline, not used in final model
# import pyspark.sql.functions as F

pcmd_silver = spark.table("pcmd_silver")

pcmd_minute_gold = (
    pcmd_silver
    .groupBy(F.window("event_ts", "1 minute"), "towerId").agg(
        F.avg(F.when(F.col("ProcedureId") == "11", F.col("ProcedureDuration"))).alias("avg_dur_request_to_release_bearer"),
        F.avg(F.when(F.col("ProcedureId") == "15", F.col("ProcedureDuration"))).alias("avg_dur_notification_data_sent_to_UE"),
        F.avg(F.when(F.col("ProcedureId") == "16", F.col("ProcedureDuration"))).alias("avg_dur_request_to_setup_bearer"),
        F.max(F.when(F.col("ProcedureId") == "11", F.col("ProcedureDuration"))).alias("max_dur_request_to_release_bearer"),
        F.max(F.when(F.col("ProcedureId") == "15", F.col("ProcedureDuration"))).alias("max_dur_notification_data_sent_to_UE"),
        F.max(F.when(F.col("ProcedureId") == "16", F.col("ProcedureDuration"))).alias("max_dur_request_to_setup_bearer"),
        F.min(F.when(F.col("ProcedureId") == "11", F.col("ProcedureDuration"))).alias("min_dur_request_to_release_bearer"),
        F.min(F.when(F.col("ProcedureId") == "15", F.col("ProcedureDuration"))).alias("min_dur_notification_data_sent_to_UE"),
        F.min(F.when(F.col("ProcedureId") == "16", F.col("ProcedureDuration"))).alias("min_dur_request_to_setup_bearer"),
        F.count(F.lit(1)).alias("totalRecords_PCMD"),
        F.first("window.start").alias("window_start"),
        F.first("Longitude").alias("Longitude"),
        F.first("Latitude").alias("Latitude"),
        F.first("City").alias("City"),
        F.first("County").alias("County"),
        F.first("State").alias("state"))
    .withColumn("date", F.col("window_start"))
    .select(
        "date",
        "towerId",
        "avg_dur_request_to_release_bearer",
        "avg_dur_notification_data_sent_to_UE",
        "avg_dur_request_to_setup_bearer",
        "max_dur_request_to_release_bearer",
        "max_dur_notification_data_sent_to_UE",
        "max_dur_request_to_setup_bearer",
        "min_dur_request_to_release_bearer",
        "min_dur_notification_data_sent_to_UE",
        "min_dur_request_to_setup_bearer",
        "totalRecords_PCMD",
        "Latitude",
        "Longitude",
        "City",
        "County",
        "State"))

pcmd_minute_gold.write.mode("overwrite").saveAsTable("pcmd_minute_gold")

# COMMAND ----------

# MAGIC %md
# MAGIC ## Aggregating on Larger Time Windows Through Scheduled Batch Workflows
# MAGIC As a last step in this data pipeline, hourly and daily aggregations of tower KPIs will be created as seen in the steps below. This process has been included in this Delta Live Tables pipeline for illustrative purposes but would typically be run on a batch hourly or daily basis in a real world scenario.
# MAGIC
# MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_batch.png" width="1000"/>

# COMMAND ----------

cdr_minute_gold = spark.table("cdr_minute_gold")

cdr_hour_gold = (
    cdr_minute_gold
    .groupBy(F.window("date", "1 hour"), "towerId").agg(
        F.sum(F.col("dropped")).alias("dropped"),   
        F.sum(F.col("answered")).alias("answered"), 
        F.sum(F.col("missed")).alias("missed"),
        F.sum(F.col("text")).alias("text"),
        F.sum(F.col("call")).alias("call"),
        F.sum(F.col("totalRecords_CDR")).alias("totalRecords_CDR"),
        F.first("City").alias("City"),
        F.first("County").alias("County"),
        F.first("State").alias("State"),
        F.first("Latitude").alias("Latitude"),
        F.first("Longitude").alias("Longitude"),                            
        F.first("window.start").alias("datetime")))

cdr_hour_gold.write.mode("overwrite").saveAsTable("cdr_hour_gold")

# COMMAND ----------

cdr_minute_gold = spark.table("cdr_minute_gold")

cdr_day_gold = (
    cdr_minute_gold
    .groupBy(F.window("date", "1 day"), "towerId").agg(
        F.sum(F.col("dropped")).alias("dropped"),   
        F.sum(F.col("answered")).alias("answered"), 
        F.sum(F.col("missed")).alias("missed"),
        F.sum(F.col("text")).alias("text"),
        F.sum(F.col("call")).alias("call"),
        F.sum(F.col("totalRecords_CDR")).alias("totalRecords_CDR"),
        F.first("City").alias("City"),
        F.first("County").alias("County"),
        F.first("State").alias("State"),
        F.first("Latitude").alias("Latitude"),
        F.first("Longitude").alias("Longitude"),                            
        F.first("window.start").alias("datetime")))

cdr_day_gold.write.mode("overwrite").saveAsTable("cdr_day_gold")

# COMMAND ----------

cdr_day_gold = spark.table("cdr_day_gold")

display(cdr_day_gold)

# COMMAND ----------

# MAGIC %md 
# MAGIC ## Using Databricks SQL for Analytics and Reliability Monitoring
# MAGIC Once the gold tables of different time aggregations are in place, analysis can be done in the Databricks SQL persona view. We provide a sample dashboard which shows the overall health of the network starting with the total number of calls and dropped calls, a geospatial view to analyze the problem areas, and lastly a table to see the find the details of every tower. 
# MAGIC
# MAGIC <br>
# MAGIC <br>
# MAGIC
# MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/Telcodashboard.png" width="1000"/>
