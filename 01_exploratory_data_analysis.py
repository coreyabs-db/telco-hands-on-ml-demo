# Databricks notebook source
# MAGIC %md
# MAGIC ## Telecommunications Reliability Metrics
# MAGIC
# MAGIC **Telecommunications LTE Architecture**
# MAGIC <br>
# MAGIC
# MAGIC
# MAGIC
# MAGIC The modern telecommunications network consists of the Base Station also known as the **eNodeB (Evolved Node B) for 4G networks** is the hardware that communicates directly with the **UE (User Enitity such as a Mobile Phone)**. The **MME (Mobility Management Entity)** manages the entire process from a cell phone making a connection to a network to a paging message being sent to the mobile phone.  
# MAGIC <img style="margin: auto" src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/Telco_simple.png" width="1200"/>
# MAGIC
# MAGIC **Use Case Overview**
# MAGIC * Telecommunications services collect many different forms of data to observe overall network reliability as well as to predict how best to expand the network to reach more customers. Some typical types of data collected are:
# MAGIC   - **PCMD (Per Call Measurement Data):** granular details of all network processes as MME (Mobility Management Entity) manages processes between the UE and the rest of the network
# MAGIC   - **CDR (Call Detail Records):** high level data describing call and SMS activity with fields such as phone number origin, phone number target, status of call/sms, duration, etc. 
# MAGIC * This data can be collected and used in provide a full view of the health of each cell tower in the network as well as the network as a whole. 
# MAGIC * **Note:** for this demo we will be primarily focused on CDR data but will also have a small sample of what PCMD could look like.
# MAGIC
# MAGIC **Business Impact of Solution**
# MAGIC * **Ease of Scaling:** with large amounts of data being generated by a telecommunications system, Databricks can provide the ability to scale so that the data can be reliably ingested and analyzed.  
# MAGIC * **Greater Network Reliability:** with the ability to monitor and predict dropped communications and more generally network faults, telecommunications providers can ultimately deliver better service for their customers and reduce churn.
# MAGIC
# MAGIC **Full Architecture from Ingestion to Analytics and Machine Learning**
# MAGIC
# MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_full.png" width="1000"/>

# COMMAND ----------

# MAGIC %md
# MAGIC ## Data Ingestion
# MAGIC
# MAGIC **Bronze Layer** 
# MAGIC
# MAGIC * Ingestion here starts with loading CDR and PCMD data directly from S3 using Autoloader. Though in this example JSON files are loaded into S3 from where Autoloader will then ingest these files into the bronze layer, streams from Kafka, Kinesis, etc. are supported by simply changing the "format" option on the read operation.
# MAGIC
# MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_bronze.png
# MAGIC " width="1000"/>

# COMMAND ----------

# MAGIC %md
# MAGIC Note: for demo purpose reading data as table and saved as table in UC, not using streaming, not using pcmd_minute_gold since the foreccast model is only using cdr_hour_gold

# COMMAND ----------

# MAGIC %run ./.setup

# COMMAND ----------

spark.sql(f"use schema {reliability_schema}")
print(f"for this notebook, using {reliability_schema} schema")

# COMMAND ----------

# MAGIC %md
# MAGIC The CDR bronze records were loaded as follows:
# MAGIC
# MAGIC     cdr_bronze = (
# MAGIC       spark.read.format("json")
# MAGIC       .load(f"{source_path}/CDR"))
# MAGIC
# MAGIC     cdr_bronze.write.mode("overwrite").saveAsTable("cdr_bronze")
# MAGIC
# MAGIC Let's take a look at what was loaded.

# COMMAND ----------

cdr_bronze = spark.table("cdr_bronze")
print(f"record count: {cdr_bronze.count()}")
display(cdr_bronze)

# COMMAND ----------

# MAGIC %md
# MAGIC The PCMD bronze records were loaded as follows:
# MAGIC
# MAGIC     pcmd_bronze = (
# MAGIC       spark.read.format("json")
# MAGIC       .load(f"{source_path}/PCMD"))
# MAGIC
# MAGIC     pcmd_bronze.write.mode("overwrite").saveAsTable("pcmd_bronze")
# MAGIC
# MAGIC Let's take a look at what was loaded.

# COMMAND ----------

pcmd_bronze = spark.table("pcmd_bronze")
print(f"record count: {pcmd_bronze.count()}")
display(pcmd_bronze)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Joining with Tower Data and Creating the Silver Layer 
# MAGIC
# MAGIC **Silver Layer**
# MAGIC
# MAGIC * In the silver layer, the data is refined removing nulls and duplicates while also joining tower information such as state, longitude, and latitude to allow for geospatial analysis. Stream-static joins are performed to do this with the streaming CDR and PCMD records being joined with static tower information which has been stored previously.
# MAGIC
# MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_silver.png" width="1000"/>

# COMMAND ----------

# MAGIC %md
# MAGIC The towers dataset was loaded using the following code:
# MAGIC
# MAGIC     towers = spark.read.json(f"{source_path}/cell_towers.json.gz")
# MAGIC
# MAGIC     towers = (
# MAGIC         towers
# MAGIC         .select(
# MAGIC             towers.properties.GlobalID.alias("GlobalID"), 
# MAGIC             towers.properties.LocCity.alias("City"), 
# MAGIC             towers.properties.LocCounty.alias("County"), 
# MAGIC             towers.properties.LocState.alias("State"), 
# MAGIC             towers.geometry.coordinates[0].alias("Longitude"), 
# MAGIC             towers.geometry.coordinates[1].alias("Latitude")))
# MAGIC
# MAGIC     towers.write.mode("overwrite").saveAsTable("towers")
# MAGIC
# MAGIC Let's review the towers data.

# COMMAND ----------

towers = spark.table("towers")
print(f"record count: {towers.count()}")
display(towers)

# COMMAND ----------

# MAGIC %md
# MAGIC The CDR silver table was created by joining the CDR bronze table to the towers table.
# MAGIC
# MAGIC     towers = spark.table("towers")
# MAGIC     cdr_bronze = spark.table("cdr_bronze")
# MAGIC
# MAGIC     cdr_silver = (
# MAGIC         cdr_bronze
# MAGIC         .join(towers, cdr_bronze.towerId == towers.GlobalID))
# MAGIC
# MAGIC     cdr_silver.write.mode("overwrite").saveAsTable("cdr_silver")
# MAGIC
# MAGIC Let's look at the resulting silver table.

# COMMAND ----------

cdr_silver = spark.table("cdr_silver")
print(f"record count: {cdr_silver.count()}")
display(cdr_silver)

# COMMAND ----------

# MAGIC %md
# MAGIC A similar join was performed to create the PCMD silver table.
# MAGIC
# MAGIC     pcmd_bronze = spark.table("pcmd_bronze")
# MAGIC
# MAGIC     pcmd_silver = (
# MAGIC         pcmd_bronze
# MAGIC         .join(towers, pcmd_bronze.towerId == towers.GlobalID))
# MAGIC
# MAGIC     pcmd_silver.write.mode("overwrite").saveAsTable("pcmd_silver")
# MAGIC
# MAGIC Let's look at the results.

# COMMAND ----------

pcmd_silver = spark.table("pcmd_silver")
print(f"record count: {pcmd_silver.count()}")
display(pcmd_silver)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Aggregating on Various Time Periods to Create the Gold Layer
# MAGIC With Spark **Structured Streaming** the streaming records can be automatically aggregated with stateful processing. Here the aggregation is done on 1 minute intervals and the KPIs are aggregated accordingly. Any interval can be selected here and larger time window aggregations can be done on a scheduled basis with Databricks **Workflows**. For example, the records that are aggregated here at 1 minute intervals can then be aggregated to hour long intervals with a workflow that runs every hour. 
# MAGIC
# MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_gold.png" width="1000"/>

# COMMAND ----------

# MAGIC %md
# MAGIC The gold tables are aggregates along various time windows. The first one is 
# MAGIC based on an aggregate of the detailed CDR records from the silver layer, with
# MAGIC a time window set to per minute.
# MAGIC
# MAGIC     cdr_silver = spark.table("cdr_silver")
# MAGIC
# MAGIC     cdr_minute_gold = (
# MAGIC         cdr_silver
# MAGIC         .groupBy(F.window("event_ts", "1 minute"), "towerId").agg(
# MAGIC             F.count(F.when(F.col("status") == "dropped", True)).alias("dropped"),
# MAGIC             F.count(F.when(F.col("status") == "answered", True)).alias("answered"),
# MAGIC             F.count(F.when(F.col("status") == "missed", True)).alias("missed"),
# MAGIC             F.count(F.when(F.col("type") == "text", True)).alias("text"),
# MAGIC             F.count(F.when(F.col("type") == "call", True)).alias("call"),
# MAGIC             F.count(F.lit(1)).alias("totalRecords_CDR"),
# MAGIC             F.first("window.start").alias("window_start"),
# MAGIC             F.first("Longitude").alias("Longitude"),
# MAGIC             F.first("Latitude").alias("Latitude"),
# MAGIC             F.first("City").alias("City"),
# MAGIC             F.first("County").alias("County"),
# MAGIC             F.first("State").alias("state"))
# MAGIC         .withColumn("date", F.col("window_start"))
# MAGIC         .select(
# MAGIC             "date",
# MAGIC             "towerId",
# MAGIC             "answered",
# MAGIC             "dropped",
# MAGIC             "missed",
# MAGIC             "call",
# MAGIC             "text",
# MAGIC             "totalRecords_CDR",
# MAGIC             "Latitude",
# MAGIC             "Longitude",
# MAGIC             "City",
# MAGIC             "County",
# MAGIC             "State"))
# MAGIC
# MAGIC     cdr_minute_gold.write.mode("overwrite").saveAsTable("cdr_minute_gold")
# MAGIC
# MAGIC Let's review the results.

# COMMAND ----------

cdr_minute_gold = spark.table("cdr_minute_gold")
print(f"record count: {cdr_minute_gold.count()}")
display(cdr_minute_gold)

# COMMAND ----------

# MAGIC %md
# MAGIC A similar transformation was performed for PCMD silver:
# MAGIC
# MAGIC     # refer to original dlt pipeline, not used in final model
# MAGIC     # import pyspark.sql.functions as F
# MAGIC
# MAGIC     pcmd_silver = spark.table("pcmd_silver")
# MAGIC
# MAGIC     pcmd_minute_gold = (
# MAGIC         pcmd_silver
# MAGIC         .groupBy(F.window("event_ts", "1 minute"), "towerId").agg(
# MAGIC             F.avg(F.when(F.col("ProcedureId") == "11", F.col("ProcedureDuration"))).alias("avg_dur_request_to_release_bearer"),
# MAGIC             F.avg(F.when(F.col("ProcedureId") == "15", F.col("ProcedureDuration"))).alias("avg_dur_notification_data_sent_to_UE"),
# MAGIC             F.avg(F.when(F.col("ProcedureId") == "16", F.col("ProcedureDuration"))).alias("avg_dur_request_to_setup_bearer"),
# MAGIC             F.max(F.when(F.col("ProcedureId") == "11", F.col("ProcedureDuration"))).alias("max_dur_request_to_release_bearer"),
# MAGIC             F.max(F.when(F.col("ProcedureId") == "15", F.col("ProcedureDuration"))).alias("max_dur_notification_data_sent_to_UE"),
# MAGIC             F.max(F.when(F.col("ProcedureId") == "16", F.col("ProcedureDuration"))).alias("max_dur_request_to_setup_bearer"),
# MAGIC             F.min(F.when(F.col("ProcedureId") == "11", F.col("ProcedureDuration"))).alias("min_dur_request_to_release_bearer"),
# MAGIC             F.min(F.when(F.col("ProcedureId") == "15", F.col("ProcedureDuration"))).alias("min_dur_notification_data_sent_to_UE"),
# MAGIC             F.min(F.when(F.col("ProcedureId") == "16", F.col("ProcedureDuration"))).alias("min_dur_request_to_setup_bearer"),
# MAGIC             F.count(F.lit(1)).alias("totalRecords_PCMD"),
# MAGIC             F.first("window.start").alias("window_start"),
# MAGIC             F.first("Longitude").alias("Longitude"),
# MAGIC             F.first("Latitude").alias("Latitude"),
# MAGIC             F.first("City").alias("City"),
# MAGIC             F.first("County").alias("County"),
# MAGIC             F.first("State").alias("state"))
# MAGIC         .withColumn("date", F.col("window_start"))
# MAGIC         .select(
# MAGIC             "date",
# MAGIC             "towerId",
# MAGIC             "avg_dur_request_to_release_bearer",
# MAGIC             "avg_dur_notification_data_sent_to_UE",
# MAGIC             "avg_dur_request_to_setup_bearer",
# MAGIC             "max_dur_request_to_release_bearer",
# MAGIC             "max_dur_notification_data_sent_to_UE",
# MAGIC             "max_dur_request_to_setup_bearer",
# MAGIC             "min_dur_request_to_release_bearer",
# MAGIC             "min_dur_notification_data_sent_to_UE",
# MAGIC             "min_dur_request_to_setup_bearer",
# MAGIC             "totalRecords_PCMD",
# MAGIC             "Latitude",
# MAGIC             "Longitude",
# MAGIC             "City",
# MAGIC             "County",
# MAGIC             "State"))
# MAGIC
# MAGIC     pcmd_minute_gold.write.mode("overwrite").saveAsTable("pcmd_minute_gold")
# MAGIC
# MAGIC Here are the results.

# COMMAND ----------

pcmd_minute_gold = spark.table("pcmd_minute_gold")
print(f"record count: {pcmd_minute_gold.count()}")
display(pcmd_minute_gold)

# COMMAND ----------

# MAGIC %md
# MAGIC ## Aggregating on Larger Time Windows Through Scheduled Batch Workflows
# MAGIC As a last step in this data pipeline, hourly and daily aggregations of tower KPIs will be created as seen in the steps below. This process has been included in this Delta Live Tables pipeline for illustrative purposes but would typically be run on a batch hourly or daily basis in a real world scenario.
# MAGIC
# MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/telco_pipeline_batch.png" width="1000"/>

# COMMAND ----------

# MAGIC %md
# MAGIC Next we'll show how the hourly CDR gold table was created:
# MAGIC
# MAGIC     cdr_minute_gold = spark.table("cdr_minute_gold")
# MAGIC
# MAGIC     cdr_hour_gold = (
# MAGIC         cdr_minute_gold
# MAGIC         .groupBy(F.window("date", "1 hour"), "towerId").agg(
# MAGIC             F.sum(F.col("dropped")).alias("dropped"),   
# MAGIC             F.sum(F.col("answered")).alias("answered"), 
# MAGIC             F.sum(F.col("missed")).alias("missed"),
# MAGIC             F.sum(F.col("text")).alias("text"),
# MAGIC             F.sum(F.col("call")).alias("call"),
# MAGIC             F.sum(F.col("totalRecords_CDR")).alias("totalRecords_CDR"),
# MAGIC             F.first("City").alias("City"),
# MAGIC             F.first("County").alias("County"),
# MAGIC             F.first("State").alias("State"),
# MAGIC             F.first("Latitude").alias("Latitude"),
# MAGIC             F.first("Longitude").alias("Longitude"),                            
# MAGIC             F.first("window.start").alias("datetime")))
# MAGIC
# MAGIC     cdr_hour_gold.write.mode("overwrite").saveAsTable("cdr_hour_gold")
# MAGIC
# MAGIC Along with viewing its results.

# COMMAND ----------

cdr_hour_gold = spark.table("cdr_hour_gold")
print(f"record count: {cdr_hour_gold.count()}")
display(cdr_hour_gold)

# COMMAND ----------

# MAGIC %md
# MAGIC For the CDR hourly gold table we'll also review a profile using Databricks profiler. 
# MAGIC Here we'll access it via a command, but note that you can also add it via the UI.

# COMMAND ----------

dbutils.data.summarize(cdr_hour_gold)

# COMMAND ----------

# MAGIC %md
# MAGIC Finally let's review the daily gold CDR table creation logic.
# MAGIC
# MAGIC     cdr_minute_gold = spark.table("cdr_minute_gold")
# MAGIC
# MAGIC     cdr_day_gold = (
# MAGIC         cdr_minute_gold
# MAGIC         .groupBy(F.window("date", "1 day"), "towerId").agg(
# MAGIC             F.sum(F.col("dropped")).alias("dropped"),   
# MAGIC             F.sum(F.col("answered")).alias("answered"), 
# MAGIC             F.sum(F.col("missed")).alias("missed"),
# MAGIC             F.sum(F.col("text")).alias("text"),
# MAGIC             F.sum(F.col("call")).alias("call"),
# MAGIC             F.sum(F.col("totalRecords_CDR")).alias("totalRecords_CDR"),
# MAGIC             F.first("City").alias("City"),
# MAGIC             F.first("County").alias("County"),
# MAGIC             F.first("State").alias("State"),
# MAGIC             F.first("Latitude").alias("Latitude"),
# MAGIC             F.first("Longitude").alias("Longitude"),                            
# MAGIC             F.first("window.start").alias("datetime")))
# MAGIC
# MAGIC     cdr_day_gold.write.mode("overwrite").saveAsTable("cdr_day_gold")
# MAGIC
# MAGIC And look at its results.

# COMMAND ----------

cdr_day_gold = spark.table("cdr_day_gold")
print(f"record count: {cdr_day_gold.count()}")
display(cdr_day_gold)

# COMMAND ----------

# MAGIC %md
# MAGIC Along with its profile.

# COMMAND ----------

dbutils.data.summarize(cdr_day_gold)

# COMMAND ----------

# MAGIC %md 
# MAGIC ## Using Databricks SQL for Analytics and Reliability Monitoring
# MAGIC Once the gold tables of different time aggregations are in place, analysis can be done in the Databricks SQL persona view. The solution accelerator provides a sample dashboard which shows the overall health of the network starting with the total number of calls and dropped calls, a geospatial view to analyze the problem areas, and lastly a table to see the find the details of every tower. 
# MAGIC
# MAGIC Feel free to add your own custom queries and visualizations to explore further.
# MAGIC
# MAGIC <br>
# MAGIC <br>
# MAGIC
# MAGIC <img src="https://raw.githubusercontent.com/databricks-industry-solutions/telco-reliability/main/images/Telcodashboard.png" width="1000"/>
